{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7377b817-560f-4a53-b3f3-df2ea10e9d09",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9097ea79-395e-40c2-96a2-80b3035a0788",
   "metadata": {},
   "source": [
    "## Parser for OData Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "73c07dda-1433-4b9e-a56a-1248f5fe8ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "test_cases = [\n",
    "    \"ColA eq 3\",\n",
    "    \"ColB eq 'abc'\",\n",
    "    \"ColA eq 3 and ColB eq 'abc'\",\n",
    "    \"startswith(ColA, 'hello')\",\n",
    "    \"(ColA eq 3 and ColB eq 'abc') or (ColC eq 55 and ColD = 'lel')\",\n",
    "    \"startswith(ColA, 'hello') and substringof('hello', ColB)\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "54806661-82f8-49b2-a1de-799da46be85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_odata_filter(query):\n",
    "    # lt\n",
    "    query = query.replace(' lt ', ' < ')\n",
    "    # le\n",
    "    query = query.replace(' le ', ' <= ')\n",
    "    # gt\n",
    "    query = query.replace(' gt ', ' > ')\n",
    "    # ge\n",
    "    query = query.replace(' ge ', ' >= ')\n",
    "    # eq\n",
    "    query = query.replace(' eq ', ' = ')\n",
    "    # ne\n",
    "    query = query.replace(' ne ', ' != ')\n",
    "    # startswith(column, string)\n",
    "    matches_sw = re.match('startswith\\(.*?\\)', query.lower())\n",
    "    if matches_sw:\n",
    "        span_sw = matches_sw.span()\n",
    "        sw_query = query[span_sw[0]:span_sw[1]]\n",
    "        # Extract text between brackets\n",
    "        sw_terms = re.sub('.*\\(', '', sw_query)\n",
    "        sw_terms = re.sub('\\).*', '', sw_terms)\n",
    "        sw_terms = [s.strip() for s in sw_terms.split(',')]\n",
    "        sw_terms[1] = re.sub('[^a-zA-Z0-9]', '', sw_terms[1])\n",
    "        query = re.sub(sw_query.replace('(', '\\(').replace(')', '\\)'), f\"{sw_terms[0]} LIKE '{sw_terms[1]}%'\", query)\n",
    "        \n",
    "    # substringof(string, column)\n",
    "    matches_so = re.search('substringof\\(.*?\\)', query.lower())\n",
    "    if matches_so:\n",
    "        span_so = matches_so.span()\n",
    "        so_query = query[span_so[0]:span_so[1]]\n",
    "        # Extract text between brackets\n",
    "        so_terms = re.sub('.*\\(', '', so_query)\n",
    "        so_terms = re.sub('\\).*', '', so_terms)\n",
    "        so_terms = [s.strip() for s in so_terms.split(',')]\n",
    "        so_terms[0] = re.sub('[^a-zA-Z0-9]', '', so_terms[0])\n",
    "        query = re.sub(so_query.replace('(', '\\(').replace(')', '\\)'), f\"{so_terms[1]} LIKE '%{so_terms[0]}%'\", query)\n",
    "    # day()\n",
    "    # month()\n",
    "    # year()\n",
    "    # hour()\n",
    "    # minute()\n",
    "    # second()\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1ac09972-95bc-4b99-a464-936d9f51525d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ColA LIKE 'hello%' and ColB LIKE '%hello%'\""
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_odata_filter(test_cases[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c73676-f11a-420b-bbec-f933fd64dc48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5c8f61af-a3e1-465f-b43e-64cf1c735e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColA = 3\n",
      "ColB = 'abc'\n",
      "ColA = 3 and ColB = 'abc'\n",
      "ColA LIKE 'hello%'\n",
      "(ColA = 3 and ColB = 'abc') or (ColC = 55 and ColD = 'lel')\n",
      "ColA LIKE 'hello%' and ColB LIKE '%hello%'\n"
     ]
    }
   ],
   "source": [
    "for test_case in test_cases:\n",
    "    print(parse_odata_filter(test_case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5815fee-bb9d-4189-a8d0-82fe60948e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_odata_filter(query, joins):\n",
    "  # Replace lookup column with the associated table\n",
    "  main_col_matches = re.findall('\\w+\\/', query)\n",
    "  main_col_replacements = []\n",
    "  for match in main_col_matches:\n",
    "    main_col_replacements.append(joins[match[:-1]]['table'])\n",
    "  for match, replacement in zip(main_col_matches, main_col_replacements):\n",
    "    query = re.sub(match, replacement + '.', query)\n",
    "  \n",
    "  # Replace slashes with dots\n",
    "  # query = query.replace('/', '.')\n",
    "  # lt\n",
    "  query = query.replace(' lt ', ' < ')\n",
    "  # le\n",
    "  query = query.replace(' le ', ' <= ')\n",
    "  # gt\n",
    "  query = query.replace(' gt ', ' > ')\n",
    "  # ge\n",
    "  query = query.replace(' ge ', ' >= ')\n",
    "  # eq\n",
    "  query = query.replace(' eq ', ' = ')\n",
    "  # ne\n",
    "  query = query.replace(' ne ', ' != ')\n",
    "  # startswith(column, string)\n",
    "  query = re.sub('startsWith', 'startswith', query, re.IGNORECASE)\n",
    "  matches_sw = re.findall('startswith\\(.*?\\)', query)\n",
    "  if len(matches_sw) > 0:\n",
    "    for match in matches_sw:\n",
    "      # Extract text between brackets\n",
    "      sw_terms = re.sub('.*\\(', '', match)\n",
    "      sw_terms = re.sub('\\).*', '', sw_terms)\n",
    "      sw_terms = [s.strip() for s in sw_terms.split(',')]\n",
    "      sw_terms[1] = re.sub('[^a-zA-Z0-9]', '', sw_terms[1])\n",
    "      query = re.sub(match.replace('(', '\\(').replace(')', '\\)'), f\"{sw_terms[0]} LIKE '{sw_terms[1]}%'\", query)\n",
    "      \n",
    "      print(query)\n",
    "      \n",
    "  # substringof(string, column)\n",
    "  matches_so = re.findall('substringof\\(.*?\\)', query, re.IGNORECASE)\n",
    "  if len(matches_so) > 0:\n",
    "    for match in matches_so:\n",
    "      # Extract text between brackets\n",
    "      so_terms = re.sub('.*\\(', '', match)\n",
    "      so_terms = re.sub('\\).*', '', so_terms)\n",
    "      so_terms = [s.strip() for s in so_terms.split(',')]\n",
    "      so_terms[0] = re.sub('[^a-zA-Z0-9]', '', so_terms[0])\n",
    "      query = re.sub(match.replace('(', '\\(').replace(')', '\\)'), f\"{so_terms[1]} LIKE '%{so_terms[0]}%'\", query)\n",
    "  \n",
    "  # day()\n",
    "\n",
    "  # month()\n",
    "  \n",
    "  # year()\n",
    "  \n",
    "  # hour()\n",
    "  \n",
    "  # minute()\n",
    "  \n",
    "  # second()\n",
    "  \n",
    "  return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ee5e56e-889c-42ba-a908-c5631641dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = {\n",
    "    '$select': 'Id,tableTitle,parentDatasetID/datasetTitle,parentDatasetID/dataDomain,parentDatasetID/owner',\n",
    "    '$filter': \"startswith(parentDatasetID/dataDomain,'O') and startswith(parentDatasetID/owner,'B')\",\n",
    "    '$expand': 'parentDatasetID'\n",
    "}\n",
    "\n",
    "query2 = {\n",
    "    '$select': 'Id,tableTitle,parentDatasetID/datasetTitle,parentDatasetID/dataDomain,parentDatasetID/owner',\n",
    "    '$filter': \"parentDatasetID/dataDomain eq 'Ops'\",\n",
    "    '$expand': 'parentDatasetID'\n",
    "}\n",
    "\n",
    "joins = {\n",
    "    'parentDatasetID': {\n",
    "        'table': 'dc_datasets'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21b36e5d-ecb3-4cc7-b3a6-d9242e44b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_odata_query(query):\n",
    "    output = {\n",
    "        'main_cols': [],\n",
    "        'join_cols': [],\n",
    "        'filter_query': '',\n",
    "        'expand_cols': []\n",
    "    }\n",
    "    if not query:\n",
    "        return\n",
    "    for query, value in query.items():\n",
    "        if query == '$filter':\n",
    "            output['filter_query'] = value\n",
    "        else:\n",
    "            columns = [v.strip() for v in value.split(',')]\n",
    "            if query == '$select':    \n",
    "                for column in columns:\n",
    "                    if '/' in column:\n",
    "                        output['join_cols'].append(column)\n",
    "                    else:\n",
    "                        output['main_cols'].append(column)\n",
    "            elif query == '$expand':\n",
    "                output['expand_cols'].extend(columns)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3a323af-1d46-46d2-a9ca-58d31c3d35fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'main_cols': ['Id', 'tableTitle'],\n",
       " 'join_cols': ['parentDatasetID/datasetTitle',\n",
       "  'parentDatasetID/dataDomain',\n",
       "  'parentDatasetID/owner'],\n",
       " 'filter_query': \"startswith(parentDatasetID/dataDomain,'O') and startswith(parentDatasetID/owner,'B')\",\n",
       " 'expand_cols': ['parentDatasetID']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_odata_query(query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc4007-a2f2-446f-8788-9d36156f6890",
   "metadata": {},
   "source": [
    "## Generate Fake Data\n",
    "The fake data generator creates fake domains, datasets, and the full underlying tables.\n",
    "\n",
    "- Domains: Arbitrary groups of datasets\n",
    "- Datasets\n",
    "\n",
    "The underlying tables include lookup tables, transactional tables, and metadata for all of them.\n",
    "\n",
    "### 1. Lookup Tables\n",
    "People: 1,000 people generated using `.profile()`:\n",
    "- name\n",
    "- sex\n",
    "- address\n",
    "- mail (email)\n",
    "- birthdate\n",
    "- company\n",
    "- job\n",
    "\n",
    "Job:\n",
    "- Job: Choose 50 from people/job and randomise for all\n",
    "- Job description: generate_key_thrust\n",
    "\n",
    "Company:\n",
    "- Choose 50 from people/company and randomise for all\n",
    "- Company description: generate_key_thrust\n",
    "\n",
    "### 2. Transactional Tables\n",
    "Training:\n",
    "- Date: date\n",
    "\n",
    "\n",
    "### 3. Metadata\n",
    "Domains: Ops, Manpower, Intel, Engineering, Training, Safety\n",
    "\n",
    "Datasets:\n",
    "- Title: Define list\n",
    "- Use case: Key thrusts\n",
    "- Owner: company\n",
    "- Point of contact: first_name, last_name, company_email\n",
    "\n",
    "Tables:\n",
    "- Title: Define list, dependent on dataset\n",
    "- Table description: Key thrusts\n",
    "- Update frequency: daily, weekly, monthly, quarterly\n",
    "- Site: url\n",
    "- spId: md5 hash of title + description\n",
    "\n",
    "Columns:\n",
    "- Depends on menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc2b16f-bd2d-4083-bbcc-5dfef08ffa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f112e71-7fca-415b-932f-689f55324832",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "Faker.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27027421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_key_thrust():\n",
    "  verbs = ['exploit', 'enhance', 'establish', 'develop', 'grow', 'construct']\n",
    "  return f\"{fake.bs().capitalize()} to {np.random.choice(verbs)} {fake.catch_phrase().lower()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033424cf",
   "metadata": {},
   "source": [
    "### Generate People Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "272e06b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_person():\n",
    "  fake_profile = fake.profile()\n",
    "  output = {\n",
    "    'name': fake_profile['name'],\n",
    "    'sex': fake_profile['sex'],\n",
    "    'address': fake_profile['address'],\n",
    "    'mail': fake_profile['mail'],\n",
    "    'birthdate': fake_profile['birthdate'],\n",
    "    'company': fake_profile['company'],\n",
    "    'job': fake_profile['job']\n",
    "  }\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa9f7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = []\n",
    "for _ in range(1000):\n",
    "  people_df.append(get_person())\n",
    "\n",
    "people_df = pd.DataFrame(people_df).reset_index().rename(columns={'index': 'Id'})\n",
    "\n",
    "# Get first 50 companies and proliferate to the rest\n",
    "company_choices = people_df.company.iloc[:50].tolist()\n",
    "people_df['company'] = np.random.choice(company_choices, 1000)\n",
    "\n",
    "# Get first 50 jobs and proliferate to the rest\n",
    "job_choices = people_df.job.iloc[:50].tolist()\n",
    "people_df['job'] = np.random.choice(job_choices, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1782c207",
   "metadata": {},
   "source": [
    "### Generate Jobs Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca4add54",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = people_df[['job']].drop_duplicates()\n",
    "jobs['job_description'] = [generate_key_thrust() for _ in range(jobs.shape[0])]\n",
    "jobs = jobs.reset_index(drop=True) \\\n",
    "  .reset_index() \\\n",
    "  .rename(columns={'index': 'Id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51694334-d6ef-4528-a48a-65fc9a502bcd",
   "metadata": {},
   "source": [
    "### Generate Company Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f76e1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = people_df[['company']].drop_duplicates()\n",
    "companies['company_description'] = [generate_key_thrust() for _ in range(companies.shape[0])]\n",
    "companies = companies \\\n",
    "  .reset_index(drop=True) \\\n",
    "  .reset_index() \\\n",
    "  .rename(columns={'index': 'Id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f42651",
   "metadata": {},
   "source": [
    "### Create Lookup in People Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3f984af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge jobs\n",
    "people_df = people_df \\\n",
    "  .merge(jobs[['job', 'Id']].rename(columns={'Id': 'jobId'}), left_on='job', right_on='job', how='left') \\\n",
    "  .drop('job', axis=1) \\\n",
    "  .rename(columns={'jobId': 'job'})\n",
    "\n",
    "# Merge companies\n",
    "people_df = people_df \\\n",
    "  .merge(companies[['company', 'Id']].rename(columns={'Id': 'companyId'}), left_on='company', right_on='company', how='left') \\\n",
    "  .drop('company', axis=1) \\\n",
    "  .rename(columns={'companyId': 'company'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c842c074-1b8c-43f9-b395-005907780c50",
   "metadata": {},
   "source": [
    "## Dataset with Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab84b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from faker import Faker\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "fake = Faker()\n",
    "Faker.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c83ce",
   "metadata": {},
   "source": [
    "### Generate Floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd1ee23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f106b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cols(coltype, n_samples=200, n_cols=5, min_value=None, max_value=None,\n",
    "                  missing_rates=[0.0, 0.0], ):\n",
    "  valid_coltypes = ['float', 'int', 'cat', 'text', 'bool', 'datetime']\n",
    "  assert coltype in valid_coltypes, \"Select an appropriate coltype: {valid_coltypes}\"\n",
    "  assert n_cols > 0, f\"You must generate at least one {coltype} column.\"\n",
    "  assert missing_rates[0] <= missing_rates[1] and missing_rates[0] >= 0.0 and missing_rates[1] <= 1.0, \"Set valid min and max missing rates.\"\n",
    "\n",
    "  if coltype == 'float':\n",
    "    data = [\n",
    "      [fake.pyfloat(right_digits=2, min_value=min_value, max_value=max_value) for _ in range(n_cols)] for _ in range(n_samples)\n",
    "    ]\n",
    "  elif coltype == 'int':\n",
    "    data = [\n",
    "      [fake.randomize_nb_elements(100, min=min_value, max=max_value) for _ in range(n_cols)] for _ in range(n_samples)\n",
    "    ]\n",
    "  elif coltype == 'cat':\n",
    "    data = [[fake.country_code() for _ in range(n_cols)] for _ in range(n_samples)]\n",
    "  elif coltype == 'text':\n",
    "    data = [[fake.sentence() for _ in range(n_cols)] for _ in range(n_samples)]\n",
    "  elif coltype == 'bool':\n",
    "    data = [np.random.choice([True, False], n_cols) for _ in range(n_samples)]\n",
    "  elif coltype == 'datetime':\n",
    "    data = [\n",
    "      [fake.date_between(start_date=min_value, end_date=max_value) for _ in range(n_cols)] for _ in range(n_samples)\n",
    "    ]\n",
    "  \n",
    "  # Convert to string\n",
    "  data = pd.DataFrame(data, columns=[f\"{coltype}_col{i+1}\" for i in range(n_cols)])\n",
    "  data = data.astype(str)\n",
    "\n",
    "  # Add missing values\n",
    "  col_missing_rates = np.random.uniform(low=missing_rates[0], high=missing_rates[1], size=n_cols)\n",
    "  rands = np.random.random(size=data.shape)\n",
    "  data[rands <= col_missing_rates] = ''\n",
    "\n",
    "  # Add incorrect data types\n",
    "  \n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d40f940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a table\n",
    "def generate_table(n_samples=200, n_float=1, n_int=1, n_cat=1, n_text=1, n_bool=1, n_dt=1,\n",
    "                  min_missing=0.0, max_missing=0.0, float_min=-100, float_max=100,\n",
    "                  int_min=None, int_max=None, start_date='-1y', end_date='today'):\n",
    "  \n",
    "  assert n_text > 0, \"You must generate at least one text column.\"\n",
    "  assert n_bool > 0, \"You must generate at least one boolean column.\"\n",
    "  assert n_dt > 0, \"You must generate at least one datetime column.\"\n",
    "  \n",
    "  # Generate data\n",
    "  data_float = generate_cols('float', n_samples=n_samples, n_cols=n_float,\n",
    "                              min_value=float_min, max_value=float_max,\n",
    "                              min_missing=min_missing, max_missing=max_missing)\n",
    "\n",
    "  data_int = generate_cols('int', n_samples=n_samples, n_cols=n_int,\n",
    "                            min_value=int_min, max_value=int_max,\n",
    "                            min_missing=min_missing, max_missing=max_missing)\n",
    "\n",
    "  data_cat = generate_cols('cat', n_samples=n_samples, n_cols=n_cat,\n",
    "                            min_missing=min_missing, max_missing=max_missing)\n",
    "  \n",
    "  data_text = generate_cols('text', n_samples=n_samples, n_cols=n_text,\n",
    "                            min_missing=min_missing, max_missing=max_missing)\n",
    "\n",
    "  data_bool = generate_cols('bool', n_samples=n_samples, n_cols=n_bool,\n",
    "                            min_missing=min_missing, max_missing=max_missing)\n",
    "\n",
    "  data_dt = generate_cols('datetime', n_samples=n_samples, n_cols=n_dt,\n",
    "                          min_value=start_date, max_value=end_date,\n",
    "                          min_missing=min_missing, max_missing=max_missing)\n",
    "  \n",
    "  # Create dataframe\n",
    "  df = pd.concat([data_float, data_int, data_cat, data_text, data_bool, data_dt], axis=1)\n",
    "  df  = df.reset_index(drop=True) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'index': 'Id'})\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "76e174d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float_col1       object\n",
      "int_col1         object\n",
      "cat_col1         object\n",
      "text_col1        object\n",
      "bool_col1        object\n",
      "datetime_col1    object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Id               0.000\n",
       "float_col1       0.235\n",
       "int_col1         0.265\n",
       "cat_col1         0.220\n",
       "text_col1        0.295\n",
       "bool_col1        0.275\n",
       "datetime_col1    0.180\n",
       "dtype: float64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = generate_table(min_missing=0.2, max_missing=0.4)\n",
    "d.apply(lambda x: x=='').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b919ccee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6d98e3681f001c7103873debdcb235ac5106553e5196c5e1fb8f98d1e494d07"
  },
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
